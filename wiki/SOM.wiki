#summary SOM implementation

NOTE: This is an ongoing development and therefore the documentation is not yet ready.

= Introduction =

Self-Organizing Map is  unsupervised  artificial neural network. SOM consists of nodes or neurons represented by one or two dimensional grid ,the usual arrangement of the neurons is a regular spacing in a hexagonal or rectangular grid. Each  neuron has a weight vector of the same dimension as the input data vectors.

In summary, SOM is feedforward structure based on competitive learning, the output neurons of the network compete among themselves and the closest neuron to the input data set going to be the winning neuron.
The principal goal of SOM is to transform the high dimension (arbitrary dimension) input data into one or two dimensional discrete map.
  

= Algorithm =
There are two training methods:
 * Stochastic Training.
 * Batch Training.

 == Stochastic Training.==
 The algorithm is summarized as follows:

  # *Initialization:* Randomly choosing the initial weight vectors _W,,j,,_.The only restriction is that _W,,j,,_ to be different for _j=1,2,...,n_ where _n_ is the number of neurons in the lattice.
  # *Sampling:* Draw randomly the input vector _x_ from the input data sets.
  # *Simmilarity matching:* Find the best-matching(winning) neuron _i(x)_ at time-step _t_ by using the minimum-distance criterion where
        _i(x)=arg min || x(t) - W,,j,, | , j=1,2,...,n_
  # *Updating:* Adjust the synaptic-weight vector of all excited neurons by using the update formula *_W,,j,,(t+1)=W,,j,,(t) + eta(t) h,,cj,,(t) ( x(t) - W,,j,,(t) )    ,   j=1,2,...,n_*  where _eta(t)_ is a learning rate parameter and _h,,cj,,(t)_ is the neighborhood function centered around the winning neuron _i(x)_; both _eta(t)_ and _h,,cj,,(t)_are varied dynamically during learning.
  # *Continuation:* Continue with step 2 to 4 for some iterations.

 
 == Batch Training.==
 In the Batch Training all the input data vectors are simultaneously    used to update all the weight vectors.

 The algorithm is summarized as follows:
  # Find the best-matching(winning) neuron _i_ according to 3 in Stochastic Training. 
  # Calculate the sum of vectors in each Voronoi set by:
   _S,,j,,_=sum(_x,,j,,_)   ,_j=1,2.....,nv,,i,,_. where _nv,,i,,_ is Voronoi set of unit _i_.
  # The number of sample in each Voronoi (_nv,,i,,_) set _i_.
  # W,,i,,(t+1)=sum( _h,,ij,,(t)_ _S,,j,,(t)_ ) / sum( _nv,,j,,_ _h,,ij,,(t)_ )  ,_j=1,2.....,m_. where _m,_ is number of neurons , _h,,ij,,(t)_ is a neighborhood function centered around the neuron _i_ at time _t_ and _nv,,j,,_ is  number of samples in the Voronoi set _j_.
  # Continue with step 1 to 4 for some iterations.


= Functions Roadmap =
==Training== 


 * {{{SOM_Maping}}}
  * {{{EvaluateSom}}}
   * {{{InitSom}}}
     * {{{unitCoords}}}
     * {{{RandomWeights}}}
     * {{{Sigma}}}
     * {{{Eta}}}
     * {{{unitDists}}}
       * {{{VectorDistance}}}
   * {{{StochasticTraining}}}
     * {{{FindClosest}}}
     * {{{UpdateWeights}}}
     * {{{GetNeurLab}}}
   * {{{BatchTrainig}}}
     * {{{VectorDistance}}}
     * {{{GetNeurLab}}}
   * {{{FastTestSOM}}}
     * {{{FindClosest}}}
   

   

== Testing ==


 * {{{SOMTest}}}
   * {{{FindClosest}}}
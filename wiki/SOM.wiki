#summary SOM implementation

NOTE: This is an ongoing development and therefore the documentation is not yet ready.

= Introduction =

Self-Organizing Map is  unsupervised  artificial neural network. SOM consists of nodes or neurons represented by one or two dimensional grid ,the usual arrangement of the neurons is a regular spacing in a hexagonal or rectangular grid. Each  neuron has a weight vector of the same dimension as the input data vectors.

In summary, SOM is feedforward structure based on competitive learning, the output neurons of the network compete among themselves and the closest neuron to the input data set going to be the winning neuron.
The principal goal of SOM is to transform the high dimension (arbitrary dimension) input data into one or two dimensional discrete map.

= Algorithm =
There are two training methods:
 * Stochastic Training.
 * Batch Training.

 == Stochastic Training^[1]^.==
 The algorithm is summarized as follows:

  # *Initialization:* Randomly choosing the initial weight vectors _W,,j,,_ with same dimension of input data.The only restriction is that _W,,j,,_ to be different for _j=1,2,...,n_ where _n_ is the number of neurons in the grid.
  # *Sampling:* Draw randomly the input vector _x_ from the input data sets.
  # *Similarity matching:* Find the best-matching(winning) neuron _i(x)_ at time-step _t_ by using the minimum-euclidean distance criterion where
        _i(x)=arg min || x(t) - W,,j,, | , j=1,2,...,n_
  # *Updating:* Adjust the synaptic-weight vector of all excited neurons by using the update formula *_W,,j,,(t+1)=W,,j,,(t) + eta(t) h,,cj,,(t) ( x(t) - W,,j,,(t) )    ,   j=1,2,...,n_*  where _eta(t)_ is a learning rate parameter and _h,,cj,,(t)_ is the neighborhood function centered around the winning neuron _i(x)_; both _eta(t)_ and _h,,cj,,(t)_are varied dynamically during learning.
  # *Continuation:* Continue  step 2 to 4 for some iterations.

 
 == Batch Training^[2]^.==
 In the Batch Training the weight vectors for all input data vectors are simultaneously updated.

 The algorithm is summarized as follows:
  # Randomly choosing the initial weight vectors  _W,,j,,_ with same dimension of input data.The only restriction is that _W,,j,,_ to be different for _j=1,2,...,n_ where _n_ is the number of neurons in the grid.
  # Find the best-matching(winning) neuron _i(x)_ according to 3 in Stochastic Training.
  # For each wining neuron _j_ calculate _S,,j,,_ is the sum of corresponding input data sets _x,,i,,_ to  wining neuron _j_
           _S,,j,,_=sum(_x,,i,,_)   ,_i=1,2......,m_ where _m_ is the number of the input data sets corresponding to  wining neuron _j_ and _j=1,2,...,n_ where _n_ is the number of neurons in the grid.
  # For each wining neuron _j_ calculate _A,,j,,_ is the number of input data sets _x,,i,,_ corresponding to that winning neuron _j_
  # _W(t+1)_= (_h,,ij,,(t)_  _S(t)_ ) / (  _A(t)_  _h,,ij,,(t)_ ). where _h,,ij,,(t)_ is a neighborhood function centered around the wining neuron _i_ at time _t_.
  # Continue  step 2 to 5 for some iterations.

 == Classification==

 # For a test set x, find the best-matching(winning) neuron i(x) according to 3 in Stochastic Training. 
 # Find the nearest labeled neuron to i(x).
 # The output of test set is the associated label to the nearest labeled neuron.


= Implementation =

 When SOM is selected as PatRec algorithm in the [GUI_PatRec], an additional GUI for configuration of SOM (GUI_SOM) will automatically appear. This GUI allows the selection of different Grid Shape, Neighbor Functions and Visualization of the Map.

* Grid Shape
 * Rectangular Grid
 * Hexagonal Grid

* Neighbor Function
 * Bubble
 * Gaussian
 * Cutgaussian
 * Epinchikov
 * Butter worth 2nd order

* Visualize U-matrix
 * yes
 * No 

These parameters are saved in a struct variable (_algConf_) and then added to the [GUI__PatRec] handles to later be retrieved for OfflinePatRec. See [Important_coding_changes] for more details.

==U-matrix==

 Unified distance matrix (U-matrix) is one methods to visualize high-dimensional space on a 2-D a gray-scale image . Where, it represented by the distance between each two neighbors neuron in the map which in turn represent the intermediate spot between those two neighbors. Some of these routines are borrowed from the SOM Toolbox^[2]^ and adapted to BioPatRec.


==Adding SOM into [BioPatRec]==

 * *Adding SOM rutines into [BioPatRec].*
  * In the OfflinePatRecTraining routine, call SOM_Maping routine if the selected algorithm is SOM. 
  * In OneShotPatRec routine, call SOMTest routine if the selected algorithm is SOM.  

= Functions Roadmap =

==GUI==
 
 * {{{GUI_SOM}}}

==Training== 

 * {{{SOM_Maping}}}
  * {{{EvaluateSOM}}}
   * {{{InitSOM}}}
     * {{{unitCoords}}}
     * {{{RandomWeights}}}
     * {{{unitDists}}}
       * {{{VectorDistance}}}
   * {{{StochasticTraining}}}
     * {{{Sigma}}}
     * {{{Eta}}}
     * {{{FindClosest}}}
     * {{{UpdateWeights}}}
       * {{{StochasticNeighborFunction}}}   
   * {{{BatchTrainig}}}
     * {{{VectorDistance}}}
     * {{{BatchNeighborFunction}}}
   * {{{GetNeurLab}}}
     * {{{FindClosest}}}
   * {{{FastTestSOM}}}
     * {{{FindClosest}}}
   * {{{FullTestSOM}}}
     * {{{FindClosest}}}
   * {{{showUDMatrix}}}
     * {{{createUDMat}}}
     * {{{UDMatCoords}}}
     * {{{plotUDMatrix}}}
       * {{{syntaxNeuron}}}
     * {{{getColor}}}

== Testing ==

 * {{{SOMTest}}}
   * {{{FindClosest}}}

= Credits =

Implemented and documented by Ali Fouad. This implementation is partially based on [http://www.cis.hut.fi/somtoolbox/download/ The SOM toolbox] ^[2]^

=References=
---------------------------------------------------------------------
 # Simon O. Haykin ,''Neural Networks and Learning Machines (3rd ed.)'',PEARSON, pp.(464-465),ISBN13:978-0-13-129376-2.
 # Juha Vesanto, Johan Himberg, Esa Alhoniemi and Parhankangs,''SOM Toolbox'',Helsinki University of Technology,pp.(9-11),ISBN 951-22-4951-0.

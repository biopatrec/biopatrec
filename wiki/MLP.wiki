#summary MLP implementation

= Introduction =

Multilayer Perceptron (MLP) is probably one of the best known Artificial Neural Networks (ANN) algorithms. Therefore you can find information about it in virtually any machine learning text. For general information see [http://en.wikipedia.org/wiki/Multilayer_perceptron wikipedia].

In summary, MLP is a feedforward topology which predictions depend on the weight assigned to each neuron connection. It has been shown useful to achieve non-linear discrimination, and it has been widely used in the prosthetic control research. 

= Implementation =

Settings such as learning rate and ANN topology can be done in the main function {{{ANN_Perceptron}}}. 

 * Automatic reset after a given number of iterations without convergence
 * Batch or stochastic training
 * The number of hidden layers and hidden neurons can be easily modify in the nHn matrix.
  * 1 hidden layer with 10 neurons -> nHn = 10;  
  * 3 hidden layer with 10 neurons -> nHn = [10 10 10];  


= Roadmap =

== Training ==

 * {{{ANN_Perceptron}}}
  * {{{InitANN_Perceptron}}}
  * {{{EvaluateANN}}}
  * Training algorithm:
   * Backpropagation, or
    * {{{Backpropagation}}}
   * PSO, or
    * {{{InitPSO_MLP}}}
    * {{{PSO_MLP}}}
     * {{{Pos2W}}}
     * {{{FitnessANN}}}  
      * {{{EvaluateANN}}}  
   * ...
  * {{{FastTestANN}}}
   * {{{EvaluateANN}}}
  * {{{FullTestANN}}}

== Testing ==

 * {{{MLPTest}}}
  * {{{EvaluateANN}}}
#summary Principal Component Analysis

NOTE: This is an ongoing development and therefore the documentation is not yet ready.

= Introduction =
 Principal Component Analysis (PCA) is one of the widely used Dimensionality Reduction Technique. In Pattern Recognition, Dimensionality Reduction can be categorized as Feature selection or Feature Projection. PCA is an unsupervised Feature reduction or Feature projection technique which does not require any information about class labels. The central idea of PCA is to reduce the dimensionality of large data set in to low dimensional space while preserving as much as possible variation present in the data.

 PCA is a linear transformation also called as karhunenloeve tranform (KLT). It transforms number of correlated variables into small number    of uncorrelated variables called as Principal Components (PC's).The first PC accounts much of variability in the data and the succeeding components accounts remaining variability. It is a true eigen vector based multivariate analysis reveling internal structure of the data which is best explains the variance in the data. 


= Algorithm =
PCA Algorithm is summarized as follows
  
   
   * *Step-1*: Compute the Normalization

   * *Step-2*: Compute sample Covariance matrix.

   * *Step-3*: Compute Eigen vectors and Eigen values. 

   * *Step-4*: Sort Eigen values in descending order.

   * *Step-7*: Arrange the Eigen vectors with their corresponding  eigen values in descending order.
 
   * *Step-8*: Choose first k largest eigen vectors. 

   * *Step-9*: Project the data on to k eigen vectors.

=== How to Chose k ===
   To choose k following criterion is used.
   Consider Eigenvalue is Lamda.
  âˆ‘ ^ð›Œ^i / âˆ‘^ð›Œ^j > Thershold(eg.0.9999) where i = 1,2,....k and j = 1,2,....N
                  
  
== Implementation == 

 Before perfoming Principal Component Analysis (PCA) the data should be normalized to bring the data in same scale.When performing PCA in [PatRec] the first step is selection of normalization. [PatRec] has various [Normalization] methods such that the choice of normalization method entirely depends according to [PatRec] algorithms. If one should need better classification performance using PCA feature reduction, the best preferred choice is ''Norm-Log'' [Normalization] for SOM,KNN and Discriminate Analysis. 

 The data is normalized according to [PatRec] structure where the training and validation sets are normalized.The testing set is normalized in [Accuracy_PatRec] function where, the test set is normalized using NormalizationSet and feed the output called 'X' as input to [PCATest] function.
 
 The PCA is performed on the Training data Set and reduce the dimesions of [trSets]. Save the Eigen vectors of Training set and project the Validation(vSet) and Testing set(tSet) on to that Eigen vectors.

 = Adding PCA in BioPatRec =
  * In [OfflinePatRec], add [FeatureReduction] fuction after trSets,and vSets Normalization and save Eigen Vectors in PatRec.
  * In [Accuracy_PatRec] function add [PCATest] funtion after tSets Normalzation

  
= Function roadmap =

 * *PCAFeatureReduction* 
    * {{{Algorithm for PCA}}}

 * *FeatureReduction*
    * {{{ Traning }}} 
    * {{{ Validation }}} 


 * * PCATest* 
    *  {{{ Test }}}
     
     
  
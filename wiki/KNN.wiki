#summary k-Nearest Neighbor

NOTE: This is an ongoing development and therefore the documentation is  
not yet finalized.

= Introduction =

 k-Nearest Neighbor Semi (k-NNS) is a modified algorithm of the traditional k-Nearest Neighbor (k-NN). It is a non parametric lazy learning algorithm, and one of most simple machine learning algorithms. The basic idea of k-NNS is, finding a group of k training vectors in the training set that are closest (neighbors) to the test vector, and then making a prediction for this group. So, that requires memorizing the entire training data and training label.

 The prediction of k-NNS is based on making a prediction for each class, which is the proportion of each class exists in this group, and the output of k-NNS includes only the class that got a prediction more than or equal to 0.5.
 
 The performance of k-NNS is primarily  based on the size of that group (k), where estimation of the group size  represents a knowledge of the data distribution. Different methods deal with the issue of k, but evaluating the classification accuracy of the validation's data with different values of k might be one of best estimation methods.

 In summary, k-NNS  makes the prediction based on prior knowledge of  the data distribution, which makes the classification more reliable.    

= Algorithm^[1]^ ==

 The algorithm can be summarized as below.
 # Computes the Euclidean distance _d (x', x,, j,,)_ between test vector _x'_ and the training data _x,,j,,_  _j=1,...,n_ where _n_ is the number of data in training sets.
 # Find the _k-minimum_ distance _d,,k,,_ (group of k nearest vectors).
 # Compute the number each class exist in this group.
 # Compute the proportion of each class exist in this group by dividing the number each class over k.
 # Include each class which has a prediction over 0.5 in the output.

   
= Implementation =
 * Initially, k has been chosen k=> number of the windows per movement(nW) 
 * The trainings are performed for choosing optimal k based on the lowest  Root Mean Square Error (RMSE) by evaluating  the validation's data with different values of k (k=1 to k=nW). 

= Functions Roadmap =
 ==Training==
The training procedure is only to choose optimal k.


 * {{{EvaluateKNN}}}
   * {{{InitKNN}}}
   * {{{FastTestKNN}}}
     * {{{K_NearestNeighbor}}}
       * {{{EuclidDist}}}
   * {{{FullTestKNN}}}
     * {{{K_NearestNeighbor}}}
       * {{{EuclidDist}}}
 == Testing ==

 * {{{KNNTest}}}
   * {{{K_NearestNeighbor}}}
     * {{{EuclidDist}}}



-------------------------------------------------------------------- 
=References:=
 # Xindong Wu, Vipin Kumar,J. Ross Quinlan, Joydeep Ghosh, Qiang Yang, Hiroshi Motoda ,Geoffrey J. McLachlan, Angus Ng, Bing Liu, Philip S. Yu,Zhi-Hua Zhou, Michael Steinbach, David J. Hand and Dan Steinberg, ''Top 10 algorithms in data mining'', Springer-Verlag London Limited 2007,pp.(22-23).
 